   19  mkdir ~/A2/
   20  cd A2
   21  pwd
   22  ls 
   23  cp /home/test/A1/downloaded_tweets_extend_nolf.txt ~/A2/
   24  cd
   25  cp /home/test/A1/downloaded_tweets_extend_nolf.txt ~/A2/
   26  cp /home/test/A1/downloaded_tweets_extend_original_nolf.txt ~/A2/
   27  ls
   28  cd A1
   29  ls
   30  head A1
   31  cd
   32  cd ws3
   33  ls
   34  cd
   35  ls
   36  rm -rf A2
   37  cd
   38  ls
   39  cp /home/test/A1/downloaded_tweets_extend_nolf.txt ~/A2/
   40  cp /home/test/A1/downloaded_tweets_extend_original_nolf.txt ~/A2/
   41  mkdir ~/A2/
   42  ls
   43  cp /home/test/A1/downloaded_tweets_extend_nolf.txt ~/A2/
   44  cd A2
   45  cp /home/test/A1/downloaded_hashtags_extend.csv A2
   46  ls
   47  head A2
   48  ls
   49  rm -rf A2
   50  ls
   51  cd A2
   52  cd A1
   53  ls
   54  head A1
   55  cd
   56  ls
   57  pwd
   58  mkdir ~/A2/
   59  cp /home/test/A1/downloaded_tweets_extend_nolf.txt ~/A2/
   60  cp /home/test/A1/downloaded_tweets_extend_original_nolf.txt
   61  cp /home/test/A1/downloaded_tweets_extend_original_nolf.txt ~/A2/
   62  exit
   63  ls
   64  cp /home/test/A2/download_tweets_extend_nolf.txt ~/A2/
   65  cd A2/
   66  pwd
   67  ls
   68  cp /home/test/A2/download_tweets_extend_nolf.txt ~/A2/
   69  cd A1
   70  cd
   71  cd A1/
   72  ls
   73  cp cp /home/test/A2/download_tweets_extend_nolf.txt ~/A2/
   74  cd
   75  cp /home/test/A1/downloaded_hashtags_extend.csv A2
   76  cd a2
   77  cd A2
   78  ls
   79  cp /home/test/A1/downloaded_tweets_extend_nolf.txt
   80  cp /home/test/A1/downloaded_tweets_extend_nolf.txt ~/A2/
   81  cd
   82  cd A1/
   83  ls
   84  ls -latr
   85  cp /home/test/A1/downloaded_tweets_extend.csv downloaded_tweets_extend.csv
   86  ls 
   87  cp /home/test/A1/downloaded_tweets_extend_nolf.txt downloaded_tweets_extend_nolf.txt
   88  cp /home/test/A1/downloaded_tweets_extend_original_nolf.csv downloaded_tweets_extend_original_nolf.csv
   89  cp /home/test/A1/downloaded_tweets_extend_original.csv downloaded_tweets_extend_original.csv
   90  ls
   91  cd
   92  cp /home/test/A1/downloaded_tweets_extend_nolf.txt ~/A2/
   93  cp /home/test/A1/downloaded_tweets_extend.txt ~/A2/
   94  cp downloaded_tweets_extend.csv downloaded_tweets_extend.csv_A2
   95  cd
   96  cd A1/
   97  cp downloaded_tweets_extend.csv A2
   98  cd 
   99  cd A2/
  100  ls
  101  cd
  102  cd A1
  103  ls -latr
  104  cd A1
  105  cd A1/
  106  ls
  107  cp /home/test/A1/downloaded_hashtags_extend.csv downloaded_hashtags_extend.csv
  108  ls
  109  ls -latr
  110  cd
  111  cp /home/test/A1/downloaded_tweets_extend_nolf.txt ~/A2/
  112  cp /home/test/A1/downloaded_tweets_extend_original_nolf.txt ~/A2/
  113  cd A1
  114  LS
  115  ls -late
  116  ls -latr
  117  more downloaded_hashtags_extend.csv
  118  ls
  119  cd A1/
  120  LS -LATR
  121  ls -latr
  122  head downloaded_tweets_extend.csv
  123  cd
  124  cd A2/
  125  LS
  126  ls -latr
  127  ls
  128  cs Ws1
  129  cd Ws1
  130  pwd
  131  ls
  132  cd 
  133  cd ws_2
  134  pwd
  135  ls
  136  cd
  137  mkdir ws4
  138  cd ws_2
  139  mv amazon_reviews_us_Books_v1_02.tsv.gz.1 ws4
  140  cd
  141  cd ws4
  142  ls
  143  cd ws_2
  144  ls
  145  cp amazon_reviews_us_Books_v1_02.tsv /ws4
  146  mv amazon_reviews_us_Books_v1_02.tsv /ws4
  147  cp amazon_reviews_us_Books_v1_02.tsv ~
  148  cd
  149  pwd
  150  ls
  151  cd ws4
  152  pwd
  153  ls
  154  cp amazon_reviews_us_Books_v1_02.tsv amazon_reviews_us_Books_v1_02.tsv
  155  cd
  156  pwd
  157  ls
  158  cp amazon_reviews_us_Books_v1_02.tsv.gz ws4
  159  ls
  160  cd ws4/
  161  pwd
  162  ls
  163  head amazon_reviews_us_Books_v1_02.tsv.gz
  164  rm amazon_reviews_us_Books_v1_02.tsv.gz
  165  ls
  166  cd
  167  rm amazon_reviews_us_Books_v1_02.tsv.gz
  168  ls
  169  cp amazon_reviews_us_Books_v1_02.tsv ws4
  170  cd ws4/
  171  ls
  172  head amazon_reviews_us_Books_v1_02.tsv
  173  git init
  174  alias l = " ls -latr "
  175  cd
  176  pwd
  177  vi ~/.bashrc
  178  cd ws4/
  179  mkdir {CUSTOMERS, PRODUCTS}
  180  ls
  181  grep "12257412" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >CUSTOMERS/12257412.txt
  182  mkdir CUTSOMERS
  183  grep "12257412" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >CUSTOMERS/12257412.txt
  184  mkdir CUSTOMERS/12257412.txt
  185  ls
  186  rm CUSTOMERS
  187  rm CUTSOMERS
  188  cd 
  189  rm -rf CUSTOMERS
  190  cd ws4
  191  ls
  192  rm -rf CUSTOMERS
  193  ls
  194  fgrep â€œ12257412â€ amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> CUSTOMERS/12257412.txt
  195  rmdir CUSTOMERS
  196  rm - r 
  197  re -r
  198  rm dir
  199  rmdir
  200  rmdir --help
  201  rm -rf CUSTOMERS,  PRODUCTS
  202  ls
  203  rm -rf {CUSTOMERS,  CUTSOMERS  PRODUCTS}
  204  ls
  205  mkdir CUSTOMERS
  206  ls
  207  grep â€œ12257412â€ amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> CUSTOMERS/12257412.txt
  208  grep "50732546" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> CUSTOMERS/50732546.txt
  209  grep "31048862" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> CUSTOMERS/31048862.txt
  210  mkdir PRODUCTS
  211  ls
  212  grep "0811828964" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> PRODUCTS/0811828964.txt
  213  grep "0373836635" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> PRODUCTS/0373836635.txt
  214  grep "0262181533" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> PRODUCTS/0262181533.txt
  215  vi CUSTOMERS
  216  head CUSTOMERS
  217  less CUSTOMERS
  218  exit
  219  cd ws4
  220  pwd
  221  ls
  222  cd PRODUCTS/
  223  pwd
  224  ls
  225  head 0262181533.txt
  226  cat 0262181533.txt
  227  cat 0373836635.txt
  228  0811828964.txt
  229  cat 0811828964.txt
  230  bc
  231  cd ws4
  232  ls
  233  pwd
  234  cd CUSTOMERS
  235  pwd
  236  ls
  237  more 12257412.txt
  238  cat 12257412.txt
  239  31048862.txt
  240  cat 31048862.txt
  241  man bc
  242  bc -h
  243  bc
  244  vi 31048862.txt
  245  cat 12257412.txt
  246  cat 50732546.txt
  247  clear
  248  head 50732546.txt
  249  history
  250  mkdir ws4
  251  script ws4.txt
  252  pwd
  253  ls
  254  cp amazon_reviews_us_Books_v1_02.tsv ws4
  255  cd ws4/
  256  pwd
  257  ls
  258  head amazon_reviews_us_Books_v1_02.tsv
  259  grep "50732546" grep â€œcidâ€ amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> CUSTOMERS/cid.txt
  260  cd
  261  rm -rf ws4
  262  ls
  263  git init
  264  mkdir ws4
  265  cp amazon_reviews_us_Books_v1_02.tsv ws4
  266  ls
  267  cd ws4/
  268  ls
  269  grep "50732546" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> CUSTOMERS/50732546.txt
  270  mkdir CUSTOMERS
  271  ls
  272  mkdir PRODUCTS
  273  ls
  274  grep "50732546" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> CUSTOMERS/50732546.txt
  275  grep "31048862" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> CUSTOMERS/31048862.txt
  276  grep "51964897" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> CUSTOMERS/51964897.txt
  277  grep "0373836635"amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> PRODUCTS/0373836635.txt
  278  cd ws4/
  279  ls
  280  head amazon_reviews_us_Books_v1_02.tsv
  281  grep "0811828964" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> PRODUCTS/0811828964.txt
  282  grep "0373836635" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> PRODUCTS/0373836635.txt
  283  grep "0262181533" amazon_reviews_us_Books_v1_02.tsv | cut -f 9 >> PRODUCTS/0262181533.txt
  284  cd CUSTOMERS
  285  ls
  286  awk 'BEGIN{s=0;}{s+=$1;}END{print s/NR;}' 31048862.txt
  287  awk 'BEGIN{s=0;}{s+=$1;}END{print s/NR;}' 50732546.txt
  288  awk 'BEGIN{s=0;}{s+=$1;}END{print s/NR;}' 51964897.txt
  289  cd 
  290  cd ws4
  291  cd PRODUCTS
  292  ls
  293  awk 'BEGIN{s=0;}{s+=$1;}END{print s/NR;}' 0262181533.txt
  294  awk 'BEGIN{s=0;}{s+=$1;}END{print s/NR;}' 0373836635.txt
  295  awk 'BEGIN{s=0;}{s+=$1;}END{print s/NR;}' 0811828964.txt
  296  cd
  297  cd ws4/
  298  history>cmds.log
  299  cs
  300  cd
  301  cs ws4
  302  cd ws4
  303  ls
  304  vi README
  305  script ws4_1.txt
  306  cat ws4*.txt >> ws4.combined.txt
  307  git status
  308  git add README
  309  cd ws4
  310  cp README ~
  311  cd
  312  ls
  313  gitadd README
  314  git add README
  315  git add ws4.txt
  316  git add ws4_1.txt
  317  cd ws4
  318  ls
  319  cp cmds.log ~
  320  cd
  321  git add cmds.log
  322  git status
  323  git branch -M main
  324  git commit -m "first commit"
  325  git remote add origin https://github.com/shamikamm/ws4.git
  326  git push -u origin main
  327  cd A1/
  328  pwd
  329  ls
  330  cd
  331  pwd
  332  ls
  333  rm ws4.combined.txt
  334  ls
  335  cd A1/
  336  ls
  337  cp downloaded_tweets_extend_original_nolf.tsv downloaded_tweets_extend_original_nolf.tsv
  338  cp downloaded_tweets_extend_nolf2.tsv downloaded_tweets_extend_nolf2.tsv 
  339  cd
  340  wget downloaded_tweets_extend_nolf2.tsv 
  341  cp /home/test/A1/downloaded_tweets_extend_original_nolf2.tsv ~
  342  cp /home/test/A1/downloaded_tweets_extend_nolf2.tsv ~
  343  ls
  344  cp downloaded_tweets_extend_original_nolf2.tsv A2
  345  cp downloaded_tweets_extend_nolf2.tsv A2
  346  cd A2/
  347  ls
  348  rm downloaded_hashtags_extend.csv
  349  ls
  350  cd A2/
  351  ls
  352  head downloaded_tweets_extend_nolf2.tsv
  353  head  downloaded_tweets_extend_original_nolf2.tsv
  354  awk -F"\t" '($2 == $6) {print $0}' downloaded_tweets_extend_original_nolf2.tsv | wc
  355  awk -F"\t" '($2 != $6) {print $0}' downloaded_tweets_extend_original_nolf2.tsv | wc
  356  awk -F"\t" '($2 != $6) {print $0}' downloaded_tweets_extend_original_nolf2.tsv > downloaded_tweets_extend_original_nolf2_NOBOT.tsv
  357  awk -F"\t" '($2 != $6) {print $0}' downloaded_tweets_extend_nolf2.tsv > downloaded_tweets_extend_nolf2_NOBOT.tsv
  358  ls
  359  head downloaded_tweets_extend_original_nolf2_NOBOT.tsv
  360  head downloaded_tweets_extend_nolf2_NOBOT.tsv
  361  grep retweeted downloaded_tweets_extend_nolf2_NOBOT.tsv
  362  cd A2
  363  pwd
  364  ls
  365  head -n 2 downloaded_tweets_extend_original_nolf2_NOBOT.tsv
  366  awk -F"\t" '{print $6}' downloaded_tweets_extend_original_nolf_NOBOT.tsv | head -n 10
  367  awk -F"\t" '{print $6}' downloaded_tweets_extend_original_nolf2_NOBOT.tsv | head -n 10
  368  awk -F"\t" '{print $6}' downloaded_tweets_extend_original_nolf2_NOBOT.tsv | tail -n 10
  369  head -n 5 downloaded_tweets_extend_original_nolf2_NOBOT.tsv
  370  cut -f 6 -d "\t" downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> in_reply_to_user_id.txt
  371  cut -f 6 -d " " downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> in_reply_to_user_id.txt
  372  head -n 10 in_reply_to_user_id.txt
  373  ls
  374  rm in_reply_to_user_id.txt
  375  rm downloaded_tweets_extend_nolf2.tsv
  376  rm  downloaded_tweets_extend_original_nolf2.tsv
  377  ls
  378  awk -F"\t" '{print $6}' downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> in_reply_to_user_id.txt
  379  ls
  380  head in_reply_to_user_id.txt
  381  cat in_reply_to_user_id.txt
  382  sort in_reply_to_user_id.txt | unique -c } sort -n | tail
  383  sort in_reply_to_user_id.txt | unique -c | sort -n | tail
  384  grep retweeted in_reply_to_user_id.txt
  385  grep retweeted in_reply_to_user_id.txt | head -n 10
  386  cd
  387  pwd
  388  ls
  389  cd A2/
  390  pwd
  391  ls
  392  cut -f 2,6exit
  393  exit
  394  cd A2/
  395  pwd
  396  ls
  397  head -n 5 downloaded_tweets_extend_original_nolf2_NOBOT.tsv
  398  awk -F "\t" '{print$2}' downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> authors.txt
  399  ls
  400  head -n 5 authors.txt
  401  sort authors.txt | unique -c | sort -n | tail -10
  402  sort authors.txt | uniq -c | sort -n | tail -10
  403  awk -F "\t" '{print$2}' downloaded_tweets_extend_original_nolf2_NOBOT.tsv | head -n 1-
  404  awk -F "\t" '{print$2}' downloaded_tweets_extend_original_nolf2_NOBOT.tsv | head -n 10
  405  ls
  406  rm in_reply_to_user_id.txt
  407  ls
  408  awk -F "\t" '{print$6}' downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> reply_user_id.txt
  409  ls
  410  sort reply_user_id.txt | uniq -c | sort -n | tail -10
  411  head -n downloaded_tweets_extend_nolf2_NOBOT.tsv
  412  head -5 downloaded_tweets_extend_nolf2_NOBOT.tsv
  413  ghghjls
  414  ls
  415  awk -F "\t" '{print$6}' downloaded_tweets_extend_nolf2_NOBOT.tsv >> reply.txt
  416  ls
  417  grep "retweeted" reply.txt | sort -n | tail -10
  418  grep "type=retweeted" reply.txt
  419  head reply.txt
  420  rm reply.txt
  421  ls
  422  exit
  423  ls
  424  rm cmds.log
  425  ls
  426  cd A2
  427  pwd
  428  ls
  429  ls -latr
  430  cut -d "\t" -f 2 downloaded_tweets_extend_original_nolf2_NOBOT.tsv > a.txt
  431  cut --help
  432  q
  433  ut -d "\" -f 2 downloaded_tweets_extend_original_nolf2_NOBOT.tsv > a.txt
  434  q
  435  exit
  436  wq
  437  cut -d "\" -f 2 downloaded_tweets_extend_original_nolf2_NOBOT.tsv > a.txt
  438  cut -d "\" -f 2 downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> a.txt
  439  ls
  440  cd A2/
  441  pwd
  442  ls
  443  rm a.txt
  444  rm authors.txt
  445  rm reply_user_id.txt
  446  ls
  447  cut -d "\" -f 2 downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> a.txt
  448  ls
  449  cd
  450  ls
  451   rm README
  452  ls
  453  cd A2
  454  pwd
  455  ls -latr
  456  cut -f 2 -d " " downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> a.txt
  457  ls
  458  sort a.txt |uniq - c| sort -n | tail -10
  459  head -n 5 a.txt
  460  sort -n a.txt | uniq -c | head
  461  head downloaded_tweets_extend_original_nolf2_NOBOT.tsv
  462  cut -f 2 -d "\" downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> b.txt
  463  cut -f 2 -d "/" downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> c.txt
  464  head c.txt
  465  ls
  466  rm c
  467  rm c.txt
  468  rm a.txt
  469  ls
  470  awk -F "\" '{print $2}' downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> a.txt
  471  awk -F "\t" '{print $2}' downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> a.txt
  472  ls
  473  head a.txt
  474  sort a.txt | uniq - c | sort -n | tail -10
  475  sort a.txt | uniq - c | sort -n |
  476  sort -n a.txt |uniq -c
  477  sort -n a.txt |uniq -c |tail -10
  478  sort -n a.txt |uniq -c |tail
  479  sort a.txt |uniq -c |sort -n |tail -n
  480  sort a.txt |uniq -c |sort -n |tail -15
  481  sort a.txt |uniq -c |sort -n |tail -10
  482  ls
  483  awk -F "\t" '{print $6}' downloaded_tweets_extend_original_nolf2_NOBOT.tsv >> u.txt
  484  sort u.txt |uniq -c |sort -n | tail -10
  485  head downloaded_tweets_extend_nolf2_NOBOT.tsv
  486  awk -F "\t" '{print $5}' downloaded_tweets_extend_nolf2_NOBOT.tsv >> b.txt
  487  grep "retweeted" b.txt >> d.txt
  488  sort d.txt | uniq -c | sort -n | tail -10
  489  ls
  490  head downloaded_tweets_extend_nolf2_NOBOT.tsv
  491  awk -F "\t" '{print $4}' downloaded_tweets_extend_nolf2_NOBOT.tsv >> f.txt
  492  sed '/^$/d' f.txt
  493  sort f.txt | uniq -c | sort -n | tail -30
  494  tr '[:upper:]' '[:lower:]' f.txt
  495  ls
  496  head downloaded_tweets_extend_nolf2_NOBOT.tsv
  497  ls
  498  head d.txt
  499  head b.txt
  500  head u.txt
  501  cat u.txt
  502  less
  503  less u.txt
  504  ls
  505  head f.txt
  506  awk -F "\t" '{4,5}' downloaded_tweets_extend_nolf2_NOBOT.tsv >> s.txt
  507  awk -F "\t" '{print $4,5}' downloaded_tweets_extend_nolf2_NOBOT.tsv >> s.txt
  508  head s.txt
  509  sort s.txt |uniq -c |sort -n |tail -30
  510  rm s.txt
  511  ls
  512  awk -F "\t" '{print $4, $5}' downloaded_tweets_extend_nolf2_NOBOT.tsv >> s.txt
  513  ls
  514  sort s.txt |uniq -c |sort -n |tail -30
  515  head f.txt
  516  mv d.txt f.txt
  517  head f.txt
  518  cat f.txt
  519  exit
  520  mkdir a2
  521  cp downloaded_tweets_extend_nolf2.tsv a2
  522  cp downloaded_tweets_extend_original_nolf2.tsv
  523  cp downloaded_tweets_extend_original_nolf2.tsv a2
  524  cd a2/
  525  pwd
  526  ls
  527  awk -F "\t" '{print $2}' downloaded_tweets_extend_original_nolf2.tsv >> authors.txt
  528  sort authors.txt |uniq -c |sort -n | tail -10
  529  awk -F "\t" '{print $6}'  downloaded_tweets_extend_original_nolf2.tsv >> replied.txt
  530  sort replied.txt |uniq -c |sort -n |tail -10
  531  awk -F "\t" '{print $5}' downloaded_tweets_extend_nolf2.tsv >> reference.txt
  532  head -n 5 reference.txt
  533  grep "retweeted" reference.txt >> retweeted.txt
  534  sort retweeted,txt |uniq -c |sort -n |tail -10
  535  head retweeted.txt
  536  sort retweeted.txt |uniq -c |sort -n |tail -10
  537  cut -f 4 downloaded_tweets_extend_nolf2.tsv >> hashtags.txt
  538  sort hashtags.txt |uniq -c |sort -n | tail -30
  539  ls
  540  cut -f 4,5 downloaded_tweets_extend_nolf2.tsv >> r.txt
  541  grep "retweeted" r.txt |uniq -c |sort -n |tail -30
  542  grep "replied to" r.txt |uniq -c |sort -n |tail 30
  543  grep "replied to" r.txt |uniq -c |sort -n |tail -30
  544  grep "replied_to" r.txt |uniq -c |sort -n |tail -30
  545  grep "replied_to" r.txt |uniq -c |sort -n |head -30
  546  sed '/^$/d' hashtags.txt
  547  grep "\"" hashtags.txt |uniq -c |sort -n |tail -30
  548  history
  549  script A2.txt
  550  vi A2.txt
  551  git add A2.txt
  552  git status
  553  git branch -M main
  554  git commit -m "first commit"
  555  git remote add origin https://github.com/shamikamm/A2.git
  556  git remote rm origin
  557  git remote add origin https://github.com/shamikamm/A2.git
  558  git push -u origin main
  559  exit
  560  ls
  561  rm ws4_1.txt
  562  ls
  563  rm ws4.txt
  564  ls
  565  rm ws3.txt
  566  rm a1.txt
  567  ls
  568  exit
  569  which tmux
  570  mkdir ws5
  571  pwd
  572  cd ws5
  573  pwd
  574  ls
  575  cd
  576  ls
  577  cp amazon_reviews_us_Books_v1_02.tsv ws5
  578  cd ws5
  579  ls
  580  pwd
  581  ls
  582  cd ws5/
  583  ls
  584  head amazon_reviews_us_Books_v1_02.tsv
  585  cut -f 2 -d ' ' amazon_reviews_us_Books_v1_02.tsv | sort -n | uniq -c >> customers.txt
  586  ls
  587  head n5 customers.txt
  588  more customers.txt |head
  589  tail -1000 customers.txt >> c.txt
  590  ls
  591  head c.txt
  592  cd
  593  cd ws3
  594  ls
  595  head customers.txt
  596  cd
  597  cd ws5
  598  pwd
  599  ls
  600  rm c.txt
  601  rm customers.txt
  602  ls
  603  awk -F " " '{print $2}' amazon_reviews_us_Books_v1_02.tsv |sort -n |uniq -c >> customers.txt
  604  ls
  605  head customrs.txt
  606  head customers.txt
  607  rm customers.txt
  608  awk -F " " '{print $2}' amazon_reviews_us_Books_v1_02.tsv |sort -n |uniq -c |tail -1000 >>customers.txt
  609  ls
  610  wc -l customers.txt
  611  head customers.txt
  612  for i in customers.txt
  613  awk -F " " '{print $2}' amazon_reviews_us_Books_v1_02.tsv >> c.txt
  614  sort c.txt |sort -n |uniq -c |tail -1000 >> ci.txt
  615  ls
  616  head ci.txt
  617  rm ci.txt
  618  rm c.txt
  619  ls
  620  for i in customers.txt; do 
  621  awk " " '{print $2}' customers.txt |head
  622  cut -f 2 customers.txt |head -10
  623  cut -f 1 customers.tx |head -10
  624  cut -f 1 customers.txt |head -10
  625  cut -f 2 -d " " customers.txt |head
  626  cut -f 3 customers.txt |head
  627  cut -f 3 customers.txt |tail
  628  for i in customers.txt ; do mv $i 
  629  for i in customers.txt; do mv $i $(CUSTOMERS/$i).txt; done
  630  for i in customers.txt; do mv $i $(basename $1 .txt)/($i).txt
  631  for i in customers.txt; do echo $i >> CUSTOMERS/$i .txt; done
  632  for c in customers.txt; do mv $i "${CUSTOMERS/$i/.txt}"; done
  633  pwd
  634  lss
  635  ls
  636  cs ws5/
  637  cd ws5/
  638  pwd
  639  ls
  640  awk -F " " '{print $2}' amazon_reviews_us_Books_v1_02.tsv |sort -n |uniq -c >>customers.txt
  641  ls
  642  tail customers.txt
  643  for c in customers.txt; do NEWNAME="${i/%.CUSTOMERS/.txt}"; mv -- "$i" "$NEWNAME"; done
  644  for c in customers,txt; do mv $c "${CUSTOMERS/$c.txt}"; done
  645  for c in customers.txt; do $c > "CUSTOMERS/$c.txt"; done
  646  for c in customers.txt; do $c > "CUSTOMERS${c}.txt"; done
  647  for c in customers.txt; do echo $c > $c.txt; done
  648  ls
  649  head customers.txt.txt
  650  cat CUSTOMERScustomers.txt.txt
  651  more CUSTOMERScustomers.txt.txt
  652  rm CUSTOMERScustomers.txt.txt
  653  ls
  654  more customers.txt.txt
  655  rm customers.txt.txt
  656  ls
  657  for c in customers.txt; do echo $c >> "c.txt"; done
  658  ls
  659  more c.txt
  660  git init
  661  mkdir ws5
  662  ls
  663  cp amazon_reviews_us_Books_v1_02.tsv ws5
  664  cd ws5/
  665  pwd
  666  ls
  667  head amazon_reviews_us_Books_v1_02.tsv
  668  ls
  669  awk -F " " '{print $2}' amazon_reviews_us_Books_v1_02.tsv |sort -n |uniq -c |tail -1000 >> customers.txt
  670  ls
  671  head customers.txt
  672  for c in customers.txt; do echo $c; done >> $c.txt
  673  script ws5.txt
  674  history
  675  tmux new-session -s homework
  676  ls
  677  rm Ws1
  678  rmdir Ws1
  679  rm -rf Ws1
  680  rm -rf ws_2
  681  ls
  682  rm -rf ws5
  683  rm -rf ws5.txt
  684  mkdir A3
  685  rm -rf cmds.log
  686  ls
  687  cp downloaded_tweets_extend_nolf2.tsv A3
  688  cp downloaded_tweets_extend_original_nolf2.tsv A3
  689  cd A3/
  690  ls
  691  head downloaded_tweets_extend_nolf2.tsv
  692  awk -F "\t" '{print $6}' downloaded_tweets_extend_nolf2.tsv >> in_reply_to_user_id.txt
  693  ls
  694  head in_reply_to_user_id.txt
  695  awk -F "\t" '{print $1}' |uniq -c
  696  awk -F "\t" '{print $1}' |uniq -c >> d.txt
  697  ls
  698  head d.txt
  699  rm d.txt
  700  ls
  701  rm  in_reply_to_user_id.txt
  702  ls
  703  cd A3
  704  ls
  705  awk -F " " '{print $6, $2}' downloaded_tweets_extend_original_nolf2.tsv >> l.txt
  706  ls
  707  head l.txt
  708  head downloaded_tweets_extend_original_nolf2.tsv
  709  ls
  710  awk -F " " '{print $1}' l.txt |sort -k 1n| uniq -c |awk '{if ($1 >+3){print}}'
  711  cd
  712  ls
  713  quit
  714  exit
  715  ls
  716  cd A3
  717  ls
  718  cut -f 6  downloaded_tweets_extend_original_nolf2.tsv > 6.txt
  719  ls
  720  head 6.txt
  721  tail 6.txt
  722  tail  downloaded_tweets_extend_original_nolf2.tsv
  723  head  downloaded_tweets_extend_original_nolf2.tsv
  724  clear
  725  ls
  726  head l.txt
  727  head 
  728  head downloaded_tweets_extend_original_nolf2.tsv
  729  awk '[print $1}' l.txt |uniq -c |wc -c |sort -k 1n |awk '{if($1>+3){print}}'
  730  awk '{print $1}' l.txt |uniq -c |wc -c |sort -k 1n |awk '{if($1>+3){print}}'
  731  awk '{print $1}' l.txt |uniq -c |wc -c |sort -k 1n
  732  head l.txt
  733  awk '{print $1}' l.txt |uniq -c |wc -c |sort -k 1n |awk '{if($1>=3){print}}'
  734  pwd
  735  ls
  736  cd A3/
  737  ls
  738  rm 6.txt
  739  ls
  740  pwd
  741  ls
  742  A3
  743  ls A3
  744  head l.txt
  745  ls -latr
  746  cd A3
  747  pwd
  748  ls
  749  head l.txt
  750  awk '{print $1}' downloaded_tweets_extend_original_nolf2.tsv |sort wc -c |uniq -c > w.txt
  751  awk '{print $1}' downloaded_tweets_extend_original_nolf2.tsv |wc -c |uniq -c > w.txt
  752  ls
  753  head w.txt
  754  cat w.txt
  755  more w.txt
  756  head downloaded_tweets_extend_original_nolf2.tsv
  757  awk '{print $1}' w.txt | uniq -c | sort -k 1n | awk '{ if ($1 >= 3) {print} }'
  758  awk '{print $1}' l.txt | uniq -c | sort -k 1n | awk '{ if ($1 >= 3) {print} }'
  759  ls
  760  pwd
  761  head -5 downloaded_tweets_extend_nolf2.tsv
  762  head -5 downloaded_tweets_extend_original_nolf2.tsv
  763  cut -f 6 -d " " |head
  764  cut -f 6 |head
  765  ls
  766  cd A3
  767  cut -f 6 -d " " downloaded_tweets_extend_original_nolf2.tsV |head
  768  grep "replied_to" downloaded_tweets_extend_original_nolf2.tsv
  769  clear
  770  ls
  771  grep "replied_to" downloaded_tweets_extend_original_nolf2.tsv >> a.txt
  772  ls
  773  rm l.txt
  774  rm w.txt
  775  ls
  776  a.txt |head -5
  777  head a.txt
  778  mv a.txt replied_to.txt
  779  ls
  780  awk -F "\t" '{print $6,$2}' replied_to.txt |sort -n >> directional_graph.txt
  781  ls
  782  head directional_graph.txt
  783  awk -F "\t" '{print $1}' directional_graph.txt |uniq -c |sort k1n |awk'{if($1>3){print}}' >> reply_cluster.txt
  784  awk -F "\t" '{print $1}' directional_graph.txt |uniq -c |sort k1n |awk'{if($1>3){print}}'
  785  awk -F "\t" '{print $1}' directional_graph.txt |uniq -c |sort -k1n |awk'{if($1>3){print}}'
  786  awk -F "\t" '{print $1}' directional_graph.txt |uniq -c |sort -k1n |awk'{if($1>3)}' >> reply_cluster.txt
  787  awk -F "\t" '{print $1}' directional_graph.txt |uniq -c |sort -k1n >>reply_cluster.txt
  788  ls
  789  head reply_cluster.txt
  790  awk '{if($1>3){print}}' reply_cluster.txt
  791  awk '{if($1>3){print}}' reply_cluster.txt > problem_2.txt
  792  ls
  793  head problem_2.txt
  794  rm problem_2.txt
  795  awk '{if($1>=3){print}}' reply_cluster.txt >> P2.txt
  796  ls
  797  head P2.txt
  798  exit
  799  ls
  800  cd A3/
  801  pwd
  802  ls
  803  gnuplot
  804  /etc/gnuplot-5.4.4/src/gnuplot
  805  exit
  806  ls
  807  cd A3/
  808  pwd
  809  ls
  810  wc -l P2.txt
  811  wc -l directional_graph.txt
  812  wc -l reply_cluster.txt
  813  /etc/gnuplot-5.4.4/scr/gnuplot
  814  /etc/gnuplot-5.4.4/src/gnuplot
  815  ls -latr
  816  stfp shamika@172.31.197.164
  817  exit
  818  display A3.svg
  819  display A3.svö
  820  gls
  821  ls
  822  cd A3/
  823  ls
  824  display A3.svg
  825  ls
  826  cd A3/
  827  ls
  828  display A3.svg
  829  ls
  830  display a3.xterm
  831  cd A3/
  832  pwd
  833  ls
  834  cat A3.svg
  835  clear
  836  ls
  837  /etc/gnuplot-5.4.4/src/gnuplot
  838  gnuplot
  839  /etc/gnuplot-5.4.4/src/gnuplot
  840  ls
  841  display a3.xterm
  842  mkdir ws6
  843  ls
  844  cd amazon_reviews_us_Books_v1_02.tsv ws6
  845  cp amazon_reviews_us_Books_v1_02.tsv ws6
  846  cd ws6/
  847  ls
  848  export DATETIME=`date "ws6"`
  849  ls
  850  cd ws6/
  851  ls
  852  echo DATETIME
  853  ECHO $DATETIME
  854  export $DATETIME=`date`
  855  export DATETIME=`date "+%Ym%d_%H%M%S"`
  856  env
  857  echo $DATETIME
  858  pwd
  859  ls
  860  cd A3/
  861  pwd
  862  ;s
  863  ls
  864  head P2.txt
  865  pwd
  866  ls
  867  export A3.svg
  868  ls
  869  cd A3/
  870  pwd
  871  ls
  872  cd ws6/
  873  ls
  874  cd
  875  mkdir -p /home/shamika/PRODUCTS/
  876  awk -F "\t" '{print 42}' amazon_reviews_us_Books_v1_02.tsv |sort |uniq -c|sort -nr |head -5
  877  head amazon_reviews_us_Books_v1_02.tsv
  878  awk -F "\t" '{print $4}' amazon_reviews_us_Books_v1_02.tsv |sort |uniq -c|sort -nr |head -5
  879  fgrep "0439139597" /mnt/scratch/shamika/amazon_reviews_us_Books_v1_02.tsv > /home/shamika/PRODUCTS/ 0439139597.txt
  880  ls
  881  rm -f A2.txt
  882  ls
  883  DATETIME=$(date +%Y%m%d_%H%M%S)
  884  cp /home/shamika/PRODUCTS/0439139597.txt /home/shamika/PRODUCTS/0439139597.$DATETIME.txt
  885   fgrep "0439139597" /mnt/scratch/shamika/amazon_reviews_us_Books_v1_02.tsv > /home/shamika/PRODUCTS/0439139597.txt
  886  cd
  887  ls
  888  rm PRODUCTS
  889  rm -f PRODUCTS
  890  rm -rf PRODUCTS
  891  ls
  892  cd ws6/
  893  ls
  894  mkdir -p PRODUCTS/
  895  fgrep -h "0439139597" amazon_reviews_us_Books_v1_02.tsv >> PRODUCTS/0439139597.txt
  896  ls
  897  DATETIME=$(date +%Y%m%d_%H%M%S)
  898  cp PRODUCTS/0439139597.txt PRODUCTS/0439139597.$DATETIME.txt
  899  ls
  900  cd PRODUCTS
  901  ls
  902  rm -f PRODUCTS/0439139597.LATEST.txt
  903  ln -s /home/shamika/ws6/PRODUCTS/0439139597.$DATETIME.txt /home/shamika/ws6/PRODUCTS/0439139597.LATEST.txt
  904  cat PRODUCTS/0439139597.LATEST.txt
  905  ls
  906  cat  0439139597.LATEST.txt
  907  exit
  908  ls
  909  cd ws6/
  910  cd PRODUCTS/
  911  ls
  912  rm 0439139597.LATEST.txt
  913  ls
  914  head 0439139597.20221019_021514.txt
  915  head amazon_reviews_us_Books_v1_02.tsv
  916  ls
  917  cd ws6/
  918  cd PEODUCTS
  919  cd PRODUCTS/
  920  ls
  921  hsitory -10
  922  history -10
  923  ls 
  924  cd ws6/
  925  ls
  926  head amazon_reviews_us_Books_v1_02.tsv
  927  cd PRODUCTS/
  928  ls
  929  cat new_product_review >> 0439139597.20221019_021514.txt
  930  cat >> 0439139597.20221019_021514.txt
  931  ln -s /home/shamika/ws6/PRODUCTS/0439139597.$DATETIME.txt /home/shamika/ws6/PRODUCTS/0439139597.LATEST.txt
  932  ls
  933  awk -F "\t" '{print $8}' PRODUCTS/0439139597.LATEST.txt | awk '{total+=1,count++} END {print total/count}' >PRODUCTS/0439139597.AVGRATING.txt
  934  awk -F "\t" '{print $8}' PRODUCTS/0439139597.LATEST.txt | awk '{total+=1,count++} END {print total/count}' > 0439139597.AVGRATING.txt
  935   awk -F "\t" '{print $8}' PRODUCTS/0439139597.LATEST.txt | awk '{total+=1;count++} END {print total/count}' > 0439139597.AVGRATING.txt
  936  awk -F "\t" '{print $8} PRODUCTS/0439139597.LATEST.txt > rating.txt
  937  awk -F "\t" '{print $8} PRODUCTS/0439139597.LATEST.txt >> rating.txt
  938  awk -F "\t" '{print $8}' PRODUCTS/0439139597.LATEST.txt >> rating.txt
  939  head 0439139597.LATEST.txt
  940  cat 0439139597.LATEST.txt
  941  ls
  942  head 0439139597.AVGRATING.txt
  943  head 0439139597.20221019_021514.txt
  944  cut -f 8 -d " " 0439139597.20221019_021514.txt > rating.txt
  945  ls
  946  head rating.txt
  947  rm rating.txt
  948  ls
  949  cut -f 9 -d " " 0439139597.20221019_021514.txt > rating.txt
  950  ls
  951  head rating.txt
  952  cut -f 9 -d ' ' 0439139597.20221019_021514.txt >> r.txt
  953  ls
  954  head r.txt
  955  cut -f 8 0439139597.20221019_021514.txt >> r.txt
  956  cut -f 8 0439139597.20221019_021514.txt >> s.txt
  957  ls
  958  rm rating.txt
  959  rm r.txt
  960  head s.txt
  961  cut -f 8 PRODUCTS/0439139597.LATEST.txt |awk '{total += $1; count++} END {print total/count}' >> PRODUCTS/0431.AVGRATIGN.txt
  962  cut -f 8 PRODUCTS/0439139597.LATEST.txt |awk '{total += $1; count++} END {print total/count}'
  963  ut -cat PRODUCTS/0439139597.LATEST.txt
  964  cat PRODUCTS/0439139597.LATEST.txt
  965  cat0439139597.LATEST.txt
  966  cat 0439139597.LATEST.txt
  967  cut -f 8 PRODUCTS/0439139597.20221019_021514.txt |awk '{total += $1; count++} END {print total/count}' >> avg.txt
  968  cut -f 8 PRODUCTS/0439139597.20221019_021514.txt >> a.txt
  969  head s.txt
  970  awk -F "\t" 'BEGIN{sum=0; count=0}{sum=sum+$8; count=count+1}END{print sum/count}' 0439139597.20221019_021514.txt
  971  awk -F "\t" 'BEGIN{sum=0; count=0}{sum=sum+$8; count=count+1}END{print sum/count}' 0439139597.LATEST.txt >> 0439139597.AVGRATING.txt
  972  crontab -e
  973  ls
  974  history
  975  exit
  976  git init
  977  mkdir ws6
  978  cp amazon_reviews_us_Books_v1_02.tsv ws6
  979  cd ws6/
  980  ls
  981  rm -rf cron.log
  982  ls
  983  vi worksheet6.sh
  984  ls
  985  crontab-l
  986  crontab -l
  987  cd PRODUCTS
  988  mkdir PRODUCTS
  989  fgrep "0439139597" amazon_reviews_us_Books_v1_02.tsv > PRODUCTS/0439139597.txt
  990  cd PRODUCTS/
  991  ls
  992  cp /home/shamika/ws6/PRODUCTS/0439139597.txt /home/shamika/ws6/PRODUCTS/0439139597.$DATETIME.TXT
  993  mv /home/shamika/ws6/PRODUCTS/0439139597.$DATETIME.TXT /home/shamika/ws6/PRODUCTS/0439139597.$DATETIME.txt
  994  ls
  995  echo $DATETIME
  996  vi worksheet6.sh
  997  ls
  998  history
  999  exit
 1000  mkdir ws6
 1001  cp amazon_reviews_us_Books_v1_02.tsv ws6
 1002  cd ws6
 1003  mkdir PRODUCTS
 1004  fgrep "04391139597" amazon_reviews_us_Books_v1_02.tsv > PRODUCTS/04391139597.txt
 1005  DATETIME=$(date +%Y%m%d_%H%M%S)
 1006  cd PRODUCTS
 1007  ls
 1008  cp /home/shamika/ws6/PRODUCTS/04391139597.txt /home/shamika/ws6/PRODUCTS/04391139597.$DATETIME.txt
 1009  ls
 1010  ln -s /home/shamika/ws6/PRODUCTS/04391139597.$DATETIME.txt home/shamika/ws6/PRODUCTS/04391139597.LATEST.txt
 1011  cd
 1012  cd ws6
 1013  ln -s /home/shamika/ws6/PRODUCTS/04391139597.$DATETIME.txt home/shamika/ws6/PRODUCTS/04391139597.LATEST.txt
 1014  vi worsksheet6.sh
 1015  ls
 1016  cat cron.log
 1017  crontab -e
 1018  history > cmds.log
